name: Esti
on:
  pull_request:
  push:
    branches:
      - master

permissions:
  id-token: write
  contents: read
  packages: write

jobs:
  check-secrets:
    name: Check if secrets are available.
    outputs:
      secretsavailable: ${{ steps.enablejobs.outputs.secretsavailable }}
    runs-on: ubuntu-20.04
    steps:
      - id: enablejobs
        env:
          ENABLE_NEXT_JOBS: ${{ secrets.AWS_ACCESS_KEY_ID }}
        run: |
          echo "Enable next jobs based on secrets existence: ${{ env.ENABLE_NEXT_JOBS != '' }}"
          echo "secretsavailable=${{ env.ENABLE_NEXT_JOBS != '' }}" >> $GITHUB_OUTPUT

  deploy-image:
    name: Build and push Docker image
    runs-on: ubuntu-20.04
    outputs:
      tag: ${{ steps.version.outputs.tag }}
    steps:
      - name: Extract version
        shell: bash
        run: echo "tag=sha-3bfc773" >> $GITHUB_OUTPUT
        id: version
  build-spark3-metadata-client:
    name: Build metadata client for Spark 3.x
    runs-on: ubuntu-20.04
    needs: check-secrets
    env:
      TAG: ${{ needs.deploy-image.outputs.tag }}
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
    steps:
      - name: Prepare Metaclient location for export
          # upload-artifact cannot take a working-directory option (that only
          # applies to "run" steps), so copy the compiled metaclient to a
          # known location.
        run: |
          mkdir -p ./export

      - uses: keithweaver/aws-s3-github-action@v1.0.0
        with:
          command: cp
          source: s3://treeverse-clients-us-east/lakefs-spark-client-301/0.5.1/lakefs-spark-client-301-assembly-0.5.1.jar
          destination: ./export/spark-assembly.jar
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws_region: us-east-1
      - uses: keithweaver/aws-s3-github-action@v1.0.0
        with:
          command: cp
          source: s3://treeverse-clients-us-east/lakefs-spark-client-312-hadoop3/0.5.1/lakefs-spark-client-312-hadoop3-assembly-0.5.1.jar
          destination: ./export/spark-assembly-hadoop3.jar
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws_region: us-east-1

      - name: Export Metaclient
        uses: actions/upload-artifact@v3
        with:
          name: metaclient-3x
          path: ./export/
          if-no-files-found: error
          retention-days: 7

  metadata-client-gc-spark3:
    name: Test lakeFS metadata client GC with Spark 3.x
    needs: [check-secrets, deploy-image, build-spark3-metadata-client]
    runs-on: ubuntu-20.04
    strategy:
      matrix:
        spark:
          - version: 3.2.1
            suffix: "-hadoop3"
          - version: 3.1.2
            suffix: ""
          - version: 3.0.2
            suffix: ""
    env:
      SPARK_TAG: ${{ matrix.spark.version }}
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
      TAG: ${{ needs.deploy-image.outputs.tag }}
    steps:
      - name: Check-out code
        uses: actions/checkout@v3

      - name: Import Metaclient
        id: import_metaclient
        uses: actions/download-artifact@v3
        with:
          name: metaclient-3x
          path: test/spark/metaclient

      - name: Generate uniquifying value
        id: unique
        run: echo "value=$RANDOM" >> $GITHUB_OUTPUT

      - name: Start lakeFS for Spark tests
        uses: ./.github/actions/bootstrap-test-lakefs
        with:
          compose-directory: test/spark
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          LAKEFS_DATABASE_TYPE: postgres
          LAKEFS_BLOCKSTORE_TYPE: s3
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID: ${{ secrets.ESTI_AWS_ACCESS_KEY_ID }}
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY: ${{ secrets.ESTI_AWS_SECRET_ACCESS_KEY }}

      - name: Setup lakeFS for metadata client tests
        working-directory: test/spark
        run: ./setup-test.sh

      - name: Test GC with Spark 3.x
        env:
          LAKEFS_BLOCKSTORE_TYPE: s3
          LAKEFS_DATABASE_TYPE: postgres
          STORAGE_NAMESPACE: s3://esti-system-testing/${{ github.run_number }}-spark${{ matrix.spark }}-metaclient/garbage/${{ steps.unique.outputs.value }}
          REPOSITORY: test-data-gc-${{ matrix.spark.version }}
          CLIENT_JAR: ${{steps.import_metaclient.outputs.download-path}}/spark-assembly${{ matrix.spark.suffix }}.jar
        working-directory: test/spark
        run: ./run-gc-test.sh

      - name: Spark executor logs on failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=50000 spark-worker

      - name: Spark driver logs on failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=50000 spark ; docker ps

      - name: lakeFS Logs on Spark with gateway failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

  metadata-client-gc-spark3-hadoop3-on-azure:
    name: Test lakeFS metadata client GC with Spark 3.x and Hadoop 3 on Azure
    needs: [check-secrets, deploy-image, build-spark3-metadata-client]
    runs-on: ubuntu-20.04
    strategy:
      matrix:
        spark:
          - version: 3.2.1
            suffix: "-hadoop3"
    env:
      SPARK_TAG: ${{ matrix.spark.version }}
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
      TAG: ${{ needs.deploy-image.outputs.tag }}
    steps:
      - name: Check-out code
        uses: actions/checkout@v3

      - name: Import Metaclient
        id: import_metaclient
        uses: actions/download-artifact@v3
        with:
          name: metaclient-3x
          path: test/spark/metaclient

      - name: Generate uniquifying value
        id: unique
        run: echo "value=$RANDOM" >> $GITHUB_OUTPUT

      - name: Start lakeFS for Spark tests
        uses: ./.github/actions/bootstrap-test-lakefs
        with:
          compose-directory: test/spark
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          LAKEFS_DATABASE_TYPE: postgres
          LAKEFS_BLOCKSTORE_TYPE: azure
          LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT: esti
          LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY: ${{ secrets.LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY2 }}

      - name: Setup lakeFS for metadata client tests
        working-directory: test/spark
        run: ./setup-test.sh

      - name: Test GC with Spark 3.x
        env:
          STORAGE_NAMESPACE: https://esti.blob.core.windows.net/esti-system-testing/${{ github.run_number }}-spark${{ matrix.spark }}-metaclient/garbage/${{ steps.unique.outputs.value }}
          REPOSITORY: test-data-gc-${{ matrix.spark.version }}
          CLIENT_JAR: ${{steps.import_metaclient.outputs.download-path}}/spark-assembly${{ matrix.spark.suffix }}.jar
          LAKEFS_BLOCKSTORE_TYPE: azure
          LAKEFS_DATABASE_TYPE: postgres
          LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT: esti
          LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY: ${{ secrets.LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY2 }}
        working-directory: test/spark
        run: ./run-gc-test.sh

      - name: lakeFS Logs on Spark with gateway failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

